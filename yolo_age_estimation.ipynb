{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7c8228",
   "metadata": {},
   "source": [
    "# Age Estimation using the YOLO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d7482",
   "metadata": {},
   "source": [
    "Authors: Isak Killingrød, Jon A B Larssen, Jon I J Skånøy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca1066",
   "metadata": {},
   "source": [
    "About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422be640",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f23cd",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1450af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run installation\n",
    "\n",
    "\n",
    "# Server\n",
    "#!python -m pip install --upgrade pip \n",
    "#!python -m pip install requests tqdm torch torchvision pandas matplotlib pillow numpy opencv-python ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef86e36-d6f5-45ca-b0a4-a5512e97d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb181961",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601ed689",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'adiencedb'\n",
    "PASSWORD = 'adience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb79e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d9c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/'\n",
    "\n",
    "ARCHIVE_URL = BASE_URL + \"faces.tar.gz\"\n",
    "\n",
    "CASCADE_FILENAME = \"haarcascade_frontalface_default.xml\"\n",
    "CASCADE_URL = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0dcd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGE_CATEGORIES = [\n",
    "    (0, 2), (4, 6), (8, 12), (15, 20), (25, 32), \n",
    "    (38, 43), \n",
    "    (48, 53), (60, 100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc03f11",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2c9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm.notebook import tqdm\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from ultralytics import YOLO\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ec8d7",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8c5d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Linux\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Windows\":\n",
    "    print(\"Running on Windows\")\n",
    "    STORAGE = 'YOLO_NB_LOCAL'\n",
    "    MODEL_SIZES = ['n', 's', 'm', 'l','x']\n",
    "    MODEL_VERSIONS = [8,9,10,11,12]\n",
    "    IMAGE_SIZES = [320,416,640]\n",
    "elif platform.system() == \"Linux\":\n",
    "    print(\"Running on Linux\")\n",
    "    STORAGE = 'YOLO_NB_SERVER'\n",
    "    MODEL_SIZES = ['x', 'l', 'm', 's', 'n']\n",
    "    MODEL_VERSIONS = [12,11,10,9,8]\n",
    "    IMAGE_SIZES = [640,416,320]\n",
    "else:\n",
    "    print(f\"Running on {platform.system()}\")\n",
    "    STORAGE = 'YOLO_NB_UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e96688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261320da-0e61-4990-b4c4-69ad8a95a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = os.cpu_count() // 2 # For preprocessing, not tuning or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b493c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_PATH = os.path.join(DATA_DIR, \"faces.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981eaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASCADE_DIR = os.path.join(DATA_DIR, \"haarcascades\")\n",
    "os.makedirs(CASCADE_DIR, exist_ok=True)\n",
    "CASCADE_PATH = os.path.join(CASCADE_DIR, CASCADE_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c44df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149892ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_with_bbox(img_path, label_path):\n",
    "    \"\"\"Display an image with its bounding box\"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(img_path)\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    # Load label\n",
    "    with open(label_path, 'r') as f:\n",
    "        line = f.readline().strip().split()\n",
    "        class_id = int(line[0])\n",
    "        x_center, y_center, width, height = map(float, line[1:5])\n",
    "    \n",
    "    # Convert YOLO format to pixel coordinates\n",
    "    x1 = int((x_center - width/2) * img_w)\n",
    "    y1 = int((y_center - height/2) * img_h)\n",
    "    x2 = int((x_center + width/2) * img_w)\n",
    "    y2 = int((y_center + height/2) * img_h)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2))\n",
    "    \n",
    "    # Label with age category\n",
    "    age_category = creator.age_categories[class_id]\n",
    "    plt.title(f\"Age Category: {age_category[0]}-{age_category[1]} years\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43bd1706-6a1f-46ae-a76d-253ef997ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_filename(version, size):\n",
    "    \"\"\"\n",
    "    Returns the correct YOLO model filename based on version and size.\n",
    "    Only supports detection models.\n",
    "\n",
    "    Args:\n",
    "        version (int or str): YOLO version (8–12)\n",
    "        size (str): Model size, depends on version\n",
    "\n",
    "    Returns:\n",
    "        str: Filename of the model checkpoint, e.g. 'yolov9e.pt'\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If version or size is unsupported\n",
    "    \"\"\"\n",
    "    version = str(version).lower()\n",
    "    size = size.lower()\n",
    "\n",
    "    if version == '9':\n",
    "        model_map = {\n",
    "            'n': 'yolov9t.pt',\n",
    "            's': 'yolov9s.pt',\n",
    "            'm': 'yolov9m.pt',\n",
    "            'l': 'yolov9c.pt',\n",
    "            'x': 'yolov9e.pt',\n",
    "        }\n",
    "        return model_map[size]\n",
    "\n",
    "    if version in ['8', '10']:\n",
    "        return f'yolov{version}{size}.pt'\n",
    "    elif version in ['11', '12']:\n",
    "        return f'yolo{version}{size}.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ad8f7f1-47d6-4bf2-b29c-f547ac0bd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data_yaml_paths(base_dir):\n",
    "    return sorted([\n",
    "        str(p) for p in Path(base_dir).rglob(\"data.yaml\")\n",
    "        if \"size_\" in str(p)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6463c623-a0fc-4285-a041-2eafbc528d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_datasets_optuna_and_training(\n",
    "    base_dataset_dir='data/age_dataset',\n",
    "    model_sizes=['n', 's', 'm', 'l', 'x'],\n",
    "    model_versions=[8, 9, 10, 11, 12],\n",
    "    image_sizes=[320, 416, 640],\n",
    "    n_trials=10,\n",
    "    epochs_per_trial=40,\n",
    "    final_epochs=100,\n",
    "    device='0',\n",
    "    output_base='runs/age_exp'\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid search over datasets, model versions, and model sizes with Optuna tuning.\n",
    "\n",
    "    Args:\n",
    "        base_dataset_dir: Root dir with YOLO-style datasets (per image size)\n",
    "        model_sizes: List of model sizes per version (e.g., ['n', 's', ...])\n",
    "        model_versions: List of YOLO model versions (e.g., [8, 9, 10, ...])\n",
    "        image_sizes: List of dataset image sizes to include (e.g., [320, 416, 640])\n",
    "        n_trials: Optuna trials per combination\n",
    "        epochs_per_trial: Epochs during Optuna tuning\n",
    "        final_epochs: Final training epochs\n",
    "        device: Device to use (e.g., '0', 'cpu')\n",
    "        output_base: Output directory base\n",
    "        run_final_training: If True, run final training after tuning\n",
    "    \"\"\"\n",
    "\n",
    "    for imgsz in image_sizes:\n",
    "        data_yaml = os.path.join(base_dataset_dir, f\"size_{imgsz}\", \"data.yaml\")\n",
    "\n",
    "        if not os.path.exists(data_yaml):\n",
    "            print(f\"⚠️  Skipping missing dataset: {data_yaml}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n📂 Evaluating dataset: {data_yaml}\")\n",
    "\n",
    "        for version in model_versions:\n",
    "            for size in model_sizes:\n",
    "                try:\n",
    "                    model_filename = get_model_filename(version, size)\n",
    "                except ValueError as e:\n",
    "                    print(f\"⏭️ Skipping unsupported model: YOLOv{version}-{size} ({e})\")\n",
    "                    continue\n",
    "\n",
    "                dataset_name = Path(data_yaml).parent.name\n",
    "                run_name = f\"v{version}_{size}_{imgsz}\"\n",
    "                output_dir = os.path.join(output_base, run_name)\n",
    "\n",
    "                print(f\"\\n{'='*100}\")\n",
    "                print(f\"🧪 Tuning: YOLOv{version}-{size} on dataset {dataset_name} ({imgsz}px)\")\n",
    "                print(f\"{'='*100}\")\n",
    "\n",
    "                best_params, best_value = run_optuna_tuning(\n",
    "                    data_yaml=data_yaml,\n",
    "                    model_size=size,\n",
    "                    model_version=version,\n",
    "                    output_dir=output_dir,\n",
    "                    n_trials=n_trials,\n",
    "                    epochs_per_trial=epochs_per_trial,\n",
    "                    device=device\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c49e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(model_path, data_yaml, imgsz, device, epochs_per_trial):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'lr0': trial.suggest_float('lr0', 1e-5, 1e-2, log=True),\n",
    "            'lrf': trial.suggest_float('lrf', 0.01, 0.2),\n",
    "            'momentum': trial.suggest_float('momentum', 0.8, 0.95),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-4, 1e-3, log=True),\n",
    "            'warmup_epochs': trial.suggest_int('warmup_epochs', 1, 5),\n",
    "            'warmup_momentum': trial.suggest_float('warmup_momentum', 0.5, 0.95),\n",
    "            'box': trial.suggest_float('box', 0.0, 8.0),\n",
    "            'cls': trial.suggest_float('cls', 0.0, 1.0),\n",
    "            'hsv_h': trial.suggest_float('hsv_h', 0.0, 1.0),\n",
    "            'hsv_s': trial.suggest_float('hsv_s', 0.0, 1.0),\n",
    "            'hsv_v': trial.suggest_float('hsv_v', 0.0, 1.0),\n",
    "            'degrees': trial.suggest_float('degrees', 0.0, 180.0),\n",
    "            'translate': trial.suggest_float('translate', 0.0, 1.0),\n",
    "            'scale': trial.suggest_float('scale', 0.0, 1.0),\n",
    "            'flipud': trial.suggest_float('flipud', 0.0, 1.0),\n",
    "            'fliplr': trial.suggest_float('fliplr', 0.0, 1.0),\n",
    "            'bgr': trial.suggest_float('bgr', 0.0, 1.0),\n",
    "            'mixup': trial.suggest_float('mixup', 0.0, 1.0),\n",
    "            'batch': trial.suggest_categorical('batch', [8, 16, 32]),\n",
    "            'imgsz': imgsz,\n",
    "            'optimizer': 'AdamW'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            model = YOLO(model_path)\n",
    "            results = model.train(\n",
    "                data=data_yaml,\n",
    "                cache='disk',\n",
    "                workers=1,\n",
    "                epochs=epochs_per_trial,\n",
    "                device=device,\n",
    "                verbose=False,\n",
    "                plots=True,\n",
    "                **params\n",
    "            )\n",
    "            return float(results.fitness) if hasattr(results, 'fitness') else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Trial failed with error: {e}\")\n",
    "            return 0.0\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39170be0-3b95-43f0-9476-b3a3739bb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_tuning(\n",
    "    data_yaml,\n",
    "    model_size='n',\n",
    "    output_dir='runs/tune_optuna',\n",
    "    n_trials=40,\n",
    "    epochs_per_trial=50,\n",
    "    model_version=8,\n",
    "    device='0'\n",
    "):\n",
    "    import warnings\n",
    "    from optuna.trial import TrialState\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    with open(data_yaml, 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "\n",
    "    dataset_dir = os.path.dirname(os.path.abspath(data_yaml))\n",
    "    size_match = re.search(r\"size_(\\d+)\", dataset_dir)\n",
    "    imgsz = int(size_match.group(1)) if size_match else 416\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_name = f\"optuna_v{model_version}_{model_size}_{imgsz}_{timestamp}\"\n",
    "    output_path = os.path.join(output_dir, run_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        model_path = get_model_filename(model_version, model_size)\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Invalid model request: {e}\")\n",
    "        return {}, 0.0\n",
    "\n",
    "    study_name = f\"yolo_v{model_version}{model_size}_img{imgsz}\"\n",
    "    study_storage = f\"sqlite:///{STORAGE}.db\"\n",
    "\n",
    "    # 📍 Query the database manually without creating a study\n",
    "    optuna_study_summaries = optuna.study.get_all_study_summaries(storage=study_storage)\n",
    "    existing_study = None\n",
    "    for s in optuna_study_summaries:\n",
    "        if s.study_name == study_name:\n",
    "            existing_study = s\n",
    "            break\n",
    "\n",
    "    if existing_study:\n",
    "        existing_trials = existing_study.n_trials\n",
    "    else:\n",
    "        existing_trials = 0\n",
    "\n",
    "\n",
    "    if existing_trials >= n_trials:\n",
    "        print(f\"⏩ Skipping tuning: {existing_trials} completed trials already (target was {n_trials}).\")\n",
    "        if existing_study:\n",
    "            study = optuna.load_study(study_name=study_name, storage=study_storage)\n",
    "            best_params = study.best_params\n",
    "            best_value = study.best_value\n",
    "        else:\n",
    "            best_params = {}\n",
    "            best_value = 0.0\n",
    "    else:\n",
    "        remaining_trials = n_trials - existing_trials\n",
    "        print(f\"🔄 Starting/resuming tuning: {remaining_trials} trials needed.\")\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=study_name,\n",
    "            storage=study_storage,\n",
    "            load_if_exists=True\n",
    "        )\n",
    "\n",
    "        objective = make_objective(model_path, data_yaml, imgsz, device, epochs_per_trial)\n",
    "        study.optimize(objective, n_trials=remaining_trials)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_value = study.best_value\n",
    "\n",
    "    with open(os.path.join(output_path, f'best_params_v{model_version}_{model_size}.json'), 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Best result for YOLOv{model_version}-{model_size} ({imgsz}px): {best_value:.4f}\")\n",
    "    return best_params, best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfdd3ee8-fd09-4302-8fe4-e0f94197ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_batch_size(model_path, imgsz, device='cuda:0', min_batch=8, max_batch=512):\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        model.model = model.model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load model {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    batch = max_batch\n",
    "    while batch >= min_batch:\n",
    "        try:\n",
    "            dummy = torch.zeros((batch, 3, imgsz, imgsz)).to(device)\n",
    "            with torch.no_grad():\n",
    "                _ = model.model(dummy)\n",
    "            return batch  \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                torch.cuda.empty_cache()\n",
    "                batch //= 2\n",
    "            else:\n",
    "                print(f\"❌ Runtime error (non-OOM) for {model_path}: {e}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Other error for {model_path}: {e}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1340c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af57eb98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FaceAgeDatasetCreator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dir=\"data\",\n",
    "        faces_archive=None,\n",
    "        faces_dir=None,\n",
    "        output_dir=None,\n",
    "        cascade_dir=None,\n",
    "        fold_files=None,\n",
    "        img_size=416,\n",
    "        max_workers=4\n",
    "    ):\n",
    "        self.base_dir = base_dir\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "        \n",
    "        self.faces_archive = faces_archive or os.path.join(base_dir, \"faces.tar.gz\")\n",
    "        self.faces_dir = faces_dir or os.path.join(base_dir, \"faces\")\n",
    "        self.output_dir = output_dir or os.path.join(base_dir, \"age_dataset\")\n",
    "        self.cascade_dir = cascade_dir or os.path.join(base_dir, \"haarcascades\")\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.cascade_dir, exist_ok=True)\n",
    "\n",
    "        if fold_files is None:\n",
    "            self.fold_files = [\n",
    "                os.path.join(base_dir, f\"fold_{i}_data.txt\") for i in range(5)\n",
    "            ]\n",
    "        else:\n",
    "            self.fold_files = fold_files\n",
    "\n",
    "        self.img_sizes = img_size if isinstance(img_size, list) else [img_size]\n",
    "        self.img_size = self.img_sizes[0]\n",
    "\n",
    "        self.age_categories = [\n",
    "            (0, 2), (4, 6), (8, 12), (15, 20), (25, 32),\n",
    "            (38, 43), (48, 53), (60, 100)\n",
    "        ]\n",
    "\n",
    "        self.max_workers = max_workers or (os.cpu_count() // 2)\n",
    "\n",
    "        try:\n",
    "            self.opencv_available = True\n",
    "            self.face_cascade = self.load_cascade()\n",
    "        except ImportError:\n",
    "            self.opencv_available = False\n",
    "            self.face_cascade = None\n",
    "            print(\"OpenCV not available. Face detection disabled.\")\n",
    "\n",
    "    def load_cascade(self):\n",
    "        paths = [\n",
    "            # cv2.data.haarcascades + 'haarcascade_frontalface_default.xml',\n",
    "            os.path.join(self.cascade_dir, 'haarcascade_frontalface_default.xml'),\n",
    "            'haarcascade_frontalface_default.xml'\n",
    "        ]\n",
    "        for path in paths:\n",
    "            if os.path.exists(path):\n",
    "                cascade = cv2.CascadeClassifier(path)\n",
    "                if cascade.empty():\n",
    "                    print(f\"Warning: Failed to load cascade from {path}\")\n",
    "                else:\n",
    "                    return cascade\n",
    "        print(\"Cascade not found or failed to load.\")\n",
    "        return None\n",
    "\n",
    "    def extract_faces_archive(self):\n",
    "        if not os.path.exists(self.faces_dir):\n",
    "            os.makedirs(self.faces_dir, exist_ok=True)\n",
    "            print(f\"Extracting {self.faces_archive} to {self.faces_dir}...\")\n",
    "            with tarfile.open(self.faces_archive, 'r:gz') as tar:\n",
    "                for member in tqdm(tar.getmembers(), desc=\"Extracting faces\"):\n",
    "                    if member.name.startswith(\"faces/\"):\n",
    "                        member.name = member.name[len(\"faces/\"):]\n",
    "                        if member.name:\n",
    "                            tar.extract(member, self.faces_dir, filter='data')\n",
    "            print(\"Extraction complete.\")\n",
    "        else:\n",
    "            print(f\"{self.faces_dir} already exists. Skipping extraction.\")\n",
    "\n",
    "    def get_age_class(self, age_info):\n",
    "        try:\n",
    "            if isinstance(age_info, str) and '(' in age_info:\n",
    "                match = re.findall(r'\\d+', age_info)\n",
    "                if len(match) >= 2:\n",
    "                    lower, upper = int(match[0]), int(match[1])\n",
    "                    for i, cat in enumerate(self.age_categories):\n",
    "                        if (lower, upper) == cat:\n",
    "                            return i\n",
    "            else:\n",
    "                age = int(age_info)\n",
    "                for i, (low, high) in enumerate(self.age_categories):\n",
    "                    if low <= age <= high:\n",
    "                        return i\n",
    "            return -1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    def load_fold_data(self, fold_files=None):\n",
    "        if fold_files is None:\n",
    "            fold_files = self.fold_files\n",
    "        all_data = []\n",
    "        column_names = [\n",
    "            'user_id', 'original_image', 'face_id', 'age', 'gender', \n",
    "            'x', 'y', 'dx', 'dy', 'tilt_ang', 'fiducial_yaw_angle', 'fiducial_score'\n",
    "        ]\n",
    "        for fold_file in fold_files:\n",
    "            try:\n",
    "                df = pd.read_csv(fold_file, sep='\\t', header=None, names=column_names)\n",
    "                df['age_class'] = df['age'].apply(self.get_age_class)\n",
    "                df = df[df['age_class'] != -1]\n",
    "                all_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {fold_file}: {e}\")\n",
    "        return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame(columns=column_names + ['age_class'])\n",
    "\n",
    "    def get_image_path(self, row):\n",
    "        filename = f\"coarse_tilt_aligned_face.{row['face_id']}.{row['original_image']}\"\n",
    "        return os.path.join(self.faces_dir, str(row['user_id']), filename)\n",
    "\n",
    "    def detect_face(self, image_np):\n",
    "        if not self.opencv_available:\n",
    "            return None\n",
    "    \n",
    "        try:\n",
    "            cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            if cascade.empty():\n",
    "                print(\"Failed to load cascade.\")\n",
    "                return None\n",
    "            gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
    "            faces = cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "            return max(faces, key=lambda r: r[2] * r[3]) if len(faces) > 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Face detection failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def is_dataset_complete(self, size_dir):\n",
    "        \"\"\"\n",
    "        Check if the dataset for a given image size is complete and ready.\n",
    "        \"\"\"\n",
    "        expected = [\n",
    "            os.path.join(size_dir, \"data.yaml\"),\n",
    "            os.path.join(size_dir, \"classes.txt\"),\n",
    "            os.path.join(size_dir, \"images/train\"),\n",
    "            os.path.join(size_dir, \"images/val\"),\n",
    "            os.path.join(size_dir, \"labels/train\"),\n",
    "            os.path.join(size_dir, \"labels/val\"),\n",
    "        ]\n",
    "        for path in expected:\n",
    "            if not os.path.exists(path):\n",
    "                return False\n",
    "        \n",
    "        val_imgs = list(Path(size_dir).joinpath(\"images/val\").glob(\"*.jpg\"))\n",
    "        val_lbls = list(Path(size_dir).joinpath(\"labels/val\").glob(\"*.txt\"))\n",
    "        \n",
    "        return len(val_imgs) > 0 and len(val_imgs) == len(val_lbls)\n",
    "\n",
    "    def process_dataset(self, data, img_dir, label_dir):\n",
    "        os.makedirs(img_dir, exist_ok=True)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        transform = transforms.Resize((self.img_size, self.img_size))\n",
    "\n",
    "        def process_row(idx_row):\n",
    "            idx, row = idx_row\n",
    "            try:\n",
    "                img_path = self.get_image_path(row)\n",
    "                if not os.path.exists(img_path):\n",
    "                    return\n",
    "        \n",
    "                with Image.open(img_path).convert('RGB') as img:\n",
    "                    orig_width, orig_height = img.size\n",
    "                    img_resized = transform(img)\n",
    "        \n",
    "                    img_np = np.array(img)\n",
    "                    img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "                    face_coords = self.detect_face(img_cv)\n",
    "        \n",
    "                    filename = os.path.basename(img_path).replace('coarse_tilt_aligned_face.', '')\n",
    "                    base_filename = f\"{idx}_{filename.split('.')[0]}\"\n",
    "                    save_path = os.path.join(img_dir, f\"{base_filename}.jpg\")\n",
    "                    img_resized.save(save_path)\n",
    "        \n",
    "                    # Handle YOLO box\n",
    "                    if face_coords is not None and isinstance(face_coords, (tuple, list, np.ndarray)):\n",
    "                        x, y, w, h = face_coords\n",
    "                        x_center = (x + w / 2) / orig_width\n",
    "                        y_center = (y + h / 2) / orig_height\n",
    "                        width_norm = w / orig_width\n",
    "                        height_norm = h / orig_height\n",
    "                    else:\n",
    "                        x_center, y_center = 0.5, 0.5\n",
    "                        width_norm, height_norm = 0.8, 0.8\n",
    "        \n",
    "                    # Clamp\n",
    "                    x_center = max(0, min(1, x_center))\n",
    "                    y_center = max(0, min(1, y_center))\n",
    "                    width_norm = max(0.05, min(1, width_norm))\n",
    "                    height_norm = max(0.05, min(1, height_norm))\n",
    "        \n",
    "                    class_id = int(row['age_class'])\n",
    "                    if not (0 <= class_id < len(self.age_categories)):\n",
    "                        print(f\"Invalid age class at index {idx}: {class_id}\")\n",
    "                        return\n",
    "        \n",
    "                    label_path = os.path.join(label_dir, f\"{base_filename}.txt\")\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width_norm:.6f} {height_norm:.6f}\\n\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error at index {idx}: {e}\")\n",
    "\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            list(tqdm(executor.map(process_row, data.iterrows()), total=len(data)))\n",
    "\n",
    "    def create_data_yaml_for_size(self, output_dir):\n",
    "        yaml_path = os.path.join(output_dir, 'data.yaml')\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            train_dir = os.path.abspath(os.path.join(output_dir, \"images/train\"))\n",
    "            val_dir = os.path.abspath(os.path.join(output_dir, \"images/val\"))\n",
    "            f.write(f\"train: {train_dir}\\n\")\n",
    "            f.write(f\"val: {val_dir}\\n\")\n",
    "            f.write(f\"nc: {len(self.age_categories)}\\n\")\n",
    "            f.write(\"names:\\n\")\n",
    "            classes_path = os.path.join(output_dir, 'classes.txt')\n",
    "            with open(classes_path, 'r') as cf:\n",
    "                for i, line in enumerate(cf):\n",
    "                    f.write(f\"  {i}: '{line.strip()}'\\n\")\n",
    "\n",
    "    def create_yolo_dataset(self, train_folds=[0, 1, 2, 3], val_fold=4, specific_sizes=None):\n",
    "        sizes_to_process = specific_sizes if specific_sizes else self.img_sizes\n",
    "        for img_size in sizes_to_process:\n",
    "            size_dir = os.path.join(self.output_dir, f\"size_{img_size}\")\n",
    "    \n",
    "            if not self.is_dataset_complete(size_dir):\n",
    "                print(f\"\\n=== Processing for image size {img_size}x{img_size} ===\")\n",
    "                self.img_size = img_size\n",
    "                img_train, img_val = os.path.join(size_dir, 'images/train'), os.path.join(size_dir, 'images/val')\n",
    "                lbl_train, lbl_val = os.path.join(size_dir, 'labels/train'), os.path.join(size_dir, 'labels/val')\n",
    "                for d in [img_train, img_val, lbl_train, lbl_val]:\n",
    "                    os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "                with open(os.path.join(size_dir, 'classes.txt'), 'w') as f:\n",
    "                    for (low, high) in self.age_categories:\n",
    "                        f.write(f\"age_{low}_{high}\\n\")\n",
    "    \n",
    "                train_data = self.load_fold_data([self.fold_files[i] for i in train_folds])\n",
    "                val_data = self.load_fold_data([self.fold_files[val_fold]])\n",
    "    \n",
    "                print(f\"Train images: {len(train_data)}, Val images: {len(val_data)}\")\n",
    "    \n",
    "                self.process_dataset(train_data, img_train, lbl_train)\n",
    "                self.process_dataset(val_data, img_val, lbl_val)\n",
    "                self.create_data_yaml_for_size(size_dir)\n",
    "                print(f\"✅ Dataset ready at: {size_dir}\")\n",
    "            else:\n",
    "                print(f\"✅ {size_dir} already complete. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9ec71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f7500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db8057e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Referer\": BASE_URL\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86127659",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b91f624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/fold_0_data.txt already exist\n",
      "data/fold_1_data.txt already exist\n",
      "data/fold_2_data.txt already exist\n",
      "data/fold_3_data.txt already exist\n",
      "data/fold_4_data.txt already exist\n"
     ]
    }
   ],
   "source": [
    "fold_files = [f\"fold_{i}_data.txt\" for i in range(5)]\n",
    "\n",
    "for fname in fold_files:\n",
    "    url = BASE_URL + fname\n",
    "    dest = os.path.join(DATA_DIR, fname)\n",
    "    \n",
    "    if os.path.exists(dest):\n",
    "        print(f\"{dest} already exist\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading {url}\")\n",
    "    \n",
    "    response = session.get(url, auth=HTTPBasicAuth(USERNAME, PASSWORD))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(dest, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Saved: {dest}\")\n",
    "    else:\n",
    "        print(f\"Failed: {url} (Status: {response.status_code})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213737f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7ddab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/faces.tar.gz already exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ARCHIVE_PATH):\n",
    "    print(f\"\\nDownloading archive: {ARCHIVE_URL}\")\n",
    "    response = session.get(ARCHIVE_URL, auth=HTTPBasicAuth(USERNAME, PASSWORD), stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        chunk_size = 8192\n",
    "\n",
    "        with open(ARCHIVE_PATH, 'wb') as f, tqdm(\n",
    "            desc=\"Downloading faces.tar.gz\",\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    bar.update(len(chunk))\n",
    "        \n",
    "        print(f\"Downloaded: {ARCHIVE_PATH}\")\n",
    "    else:\n",
    "        print(f\"Failed to download archive (Status: {response.status_code})\")\n",
    "else:\n",
    "    print(f\"{ARCHIVE_PATH} already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfac65",
   "metadata": {},
   "source": [
    "### Haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30db7e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cascade file already exists: data/haarcascades/haarcascade_frontalface_default.xml\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CASCADE_PATH):\n",
    "    print(f\"\\nDownloading Haar cascade: {CASCADE_FILENAME}\")\n",
    "    try:\n",
    "        response = requests.get(CASCADE_URL, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(CASCADE_PATH, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded cascade file to: {CASCADE_PATH}\")\n",
    "        else:\n",
    "            print(f\"Failed to download cascade file (Status: {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading cascade file: {e}\")\n",
    "else:\n",
    "    print(f\"Cascade file already exists: {CASCADE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9762e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9987cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = FaceAgeDatasetCreator(\n",
    "    base_dir=DATA_DIR,\n",
    "    img_size=[640, 416, 320],\n",
    "    max_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe056897",
   "metadata": {},
   "source": [
    "### Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18f06ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for fold files: ['data/fold_0_data.txt', 'data/fold_1_data.txt', 'data/fold_2_data.txt', 'data/fold_3_data.txt', 'data/fold_4_data.txt']\n"
     ]
    }
   ],
   "source": [
    "fold_files = creator.fold_files\n",
    "print(f\"Looking for fold files: {fold_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0834b9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17411 records from fold files\n"
     ]
    }
   ],
   "source": [
    "data = creator.load_fold_data()\n",
    "print(f\"Loaded {len(data)} records from fold files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e6e4d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>tilt_ang</th>\n",
       "      <th>fiducial_yaw_angle</th>\n",
       "      <th>fiducial_score</th>\n",
       "      <th>age_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399646885_67c7d20df9_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>1086</td>\n",
       "      <td>1383</td>\n",
       "      <td>-115</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10424815813_e94629b1ec_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>301</td>\n",
       "      <td>105</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>2395</td>\n",
       "      <td>876</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>175</td>\n",
       "      <td>-30</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>752</td>\n",
       "      <td>1255</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>11816644924_075c3d8d59_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>-75</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image face_id       age gender     x  \\\n",
       "0  30601258@N03  10399646885_67c7d20df9_o.jpg       1  (25, 32)      f     0   \n",
       "1  30601258@N03  10424815813_e94629b1ec_o.jpg       2  (25, 32)      m   301   \n",
       "2  30601258@N03  10437979845_5985be4b26_o.jpg       1  (25, 32)      f  2395   \n",
       "3  30601258@N03  10437979845_5985be4b26_o.jpg       3  (25, 32)      m   752   \n",
       "4  30601258@N03  11816644924_075c3d8d59_o.jpg       2  (25, 32)      m   175   \n",
       "\n",
       "      y    dx    dy tilt_ang fiducial_yaw_angle fiducial_score  age_class  \n",
       "0   414  1086  1383     -115                 30             17          4  \n",
       "1   105   640   641        0                  0             94          4  \n",
       "2   876   771   771      175                -30             74          4  \n",
       "3  1255   484   485      180                  0             47          4  \n",
       "4    80   769   768      -75                  0             34          4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(data) > 0:\n",
    "    print(\"\\nSample data:\")\n",
    "    display(data.head())\n",
    "    \n",
    "    # Show age distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    data['age_class'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Distribution of Age Classes')\n",
    "    plt.xlabel('Age Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38d5f4",
   "metadata": {},
   "source": [
    "### Extract faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2f0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/faces already exists. Skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "creator.extract_faces_archive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd965645",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62c34e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ data/age_dataset/size_640 already complete. Skipping...\n",
      "✅ data/age_dataset/size_416 already complete. Skipping...\n",
      "✅ data/age_dataset/size_320 already complete. Skipping...\n"
     ]
    }
   ],
   "source": [
    "creator.create_yolo_dataset(train_folds=[0, 1, 2, 3], val_fold=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a9820",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613099b5-abe6-48b8-be3f-759125dacd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Evaluating dataset: data/age_dataset/size_320/data.yaml\n",
      "\n",
      "====================================================================================================\n",
      "🧪 Tuning: YOLOv8-n on dataset size_320 (320px)\n",
      "====================================================================================================\n",
      "⏩ Skipping tuning: 5 completed trials already (target was 5).\n",
      "\n",
      "✅ Best result for YOLOv8-n (320px): 0.2749\n",
      "\n",
      "====================================================================================================\n",
      "🧪 Tuning: YOLOv8-s on dataset size_320 (320px)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 11:16:48,834] A new study created in RDB with name: yolo_v8m_img320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipping tuning: 5 completed trials already (target was 5).\n",
      "\n",
      "✅ Best result for YOLOv8-s (320px): 0.4071\n",
      "\n",
      "====================================================================================================\n",
      "🧪 Tuning: YOLOv8-m on dataset size_320 (320px)\n",
      "====================================================================================================\n",
      "🔄 Starting/resuming tuning: 5 trials needed.\n",
      "New https://pypi.org/project/ultralytics/8.3.118 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105 🚀 Python-3.10.17 torch-2.7.0+cu126 CUDA:0 (NVIDIA A100 80GB PCIe MIG 1g.20gb, 19968MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=data/age_dataset/size_320/data.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=320, save=True, save_period=-1, cache=disk, device=0, workers=1, project=None, name=train40, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.00012755230594933627, lrf=0.12776246423021145, momentum=0.8747022925801999, weight_decay=0.0009060976022691557, warmup_epochs=4, warmup_momentum=0.6708189967874958, warmup_bias_lr=0.1, box=0.20272010900625403, cls=0.7705311360370536, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.24226262714956648, hsv_s=0.5983455087402605, hsv_v=0.19608574267326995, degrees=11.472772269759338, translate=0.2017884091874339, scale=0.44182292143580726, shear=0.0, perspective=0.0, flipud=0.9768602291041014, fliplr=0.9057122420507188, bgr=0.6962177417732959, mosaic=1.0, mixup=0.4936157360282205, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train40\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3780328  ultralytics.nn.modules.head.Detect           [8, [192, 384, 576]]          \n"
     ]
    }
   ],
   "source": [
    "run_all_datasets_optuna_and_training(\n",
    "    base_dataset_dir='data/age_dataset',\n",
    "    model_sizes=MODEL_SIZES,\n",
    "    model_versions=MODEL_VERSIONS, \n",
    "    image_sizes=IMAGE_SIZES,\n",
    "    n_trials=5, # per combination\n",
    "    epochs_per_trial=10,\n",
    "    final_epochs=100,\n",
    "    device='0',\n",
    "    output_base='runs/age_exp'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yoloenv)",
   "language": "python",
   "name": "yoloenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
